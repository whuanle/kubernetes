### 导读

在前面的学习中，我们学到了 Deployment 部署，以及副本数(ReplicaSet)，但是 Pod 部署到哪个 Worker 节点是随机，即使有 3个 Woker 和设定 3个 副本，不一定每个 Node 刚刚好运行一个 Pod，也可能其中 Node 运行着三个副本。

在本篇我们将探究 Kubernetes 中的 DaemonSet、容忍度、亲和性、Label、选择器等概念，以便控制 pod 的部署。



## 标签和选择器

### 标签

标签主要是用于表示对用户有意义的对象的属性标识。

标签(Label)是附加到 Kubernetes 对象上的键值对，在对象上以 `Labels` 字段保存，例如 Pod ，通过 `kubectl describe pods` ，可以看到每个 Pod 都带有 `Labels: app=...`。

在 `kubectl get` 后面加上 `--show-labels` 可以快速筛选获得对象的 Label ：

```
kubectl get pods --show-labels
```

```
NAME                     READY   STATUS    RESTARTS   AGE     LABELS
nginx-55495b586f-4hlzm   1/1     Running   0          4h16m   app=nginx,pod-template-hash=55495b586f
nginx-55495b586f-5m985   1/1     Running   0          4h16m   app=nginx,pod-template-hash=55495b586f
nginx-55495b586f-cs4zh   1/1     Running   0          4h16m   app=nginx,pod-template-hash=55495b586f
... ...
```

在对象的 YAML 文件中，`metadata.label` 存储了这个对象的标签元数据。



如果用 JSON 表示附加到 metadata 的 label：

```json
"metadata": {
  "labels": {
    "key1" : "value1",
    "key2" : "value2"
  }
}
```

如果用 YAML 表示附加到 metadata 的 label

```shell
metadata:
  labels:
    key1: "value1"
    key2: "value2"
```



Label 可以在多个地方使用，例如在 Node 上添加 Label，标识此 Node；而在 NodeSelector 里使用，可以选择合适的 Node 运行 Pod；在 `metadata` 中使用，可以对元数据加以描述。

在 metadata 中添加的 Label，可以在命令查询时做筛选。

查询 pod 的 Label：

```shell
kubectl get pods --show-labels
```

查找符合条件的 pod（参考 LABELS 字段，可以根据里面的标签选择）：

```shell
kubectl get pods -l app=nginx
```





### 设置 Label

在 kube-system 命名空间中，运行着 Kubernetes 的核心组件，我们可以查看此命名空间中所有组件的 Label。

```shell
kubectl get nodes --namespace=kube-system --show-labels
```

```
beta.kubernetes.io/arch=amd64,
beta.kubernetes.io/os=linux,
kubernetes.io/arch=amd64,
... ...
```

我们也可以手动给一个 Node 或者别的对象 添加标签。

```shell
kubectl label nodes <node-name> <label-key>=<label-value>
kubectl label pod {pod名称} <node-name> <label-key>=<label-value>
```



例如我们给一个 Pod 设置一个 `disksize`，表示 Pod 需要很大的硬盘。

```shell
kubectl label pod nginx-55495b586f-4hlzm disksize=big
```

如果标签已经存在，需要覆盖：

```shell
kubectl label pod nginx-55495b586f-4hlzm disksize=big --overwrite
```



### nodeSelector

假如这个 Pod 需要很大的存储空间，而集群中的不同机器的硬盘容量都不同，我们希望 Pod 在硬盘很大的服务器上部署，我们可以通过给节点设置标签，然后在 Pod 的 YAML 文件中加上选择器。



获取集群中的所有节点已经显示已经存在的 Label：

```shell
kubectl get nodes
kubectl get nodes --show-labels
```

```
NAME         STATUS   ROLES                  AGE   VERSION
instance-1   Ready    control-plane,master   14d   v1.21.0
instance-2   Ready    <none>                 13d   v1.21.0

... ...
```

> 我们的节点名称不一样



给节点加上标签：

```shell
kubectl label node instance-2 disksize=big
```



然后我们在编写 yaml 文件时，希望这个 pod 在容量大的 Node 上运行，可以这样写：

```
  nodeSelector:
    disksize=big
```



先删除以前的 deployment：`kubectl delete deployment nginx`。

然后我们使用 YAML 创建一个 Deployment，保存下面 YAML 到 nginx.yaml 文件：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:latest
        name: nginx
      nodeSelector:
        disksize: "big"
```

执行 `kubectl apply -f nginx.yaml` ，部署 Pod。

上面的例子由于是 Deployment 的，selector 和别的字段是必填的，所以看起来内容比较多，但是这里核心是 `nodeSelector`。

我们需要使用节点选择器时，在 spec 下面增加 `nodeSelector`，其位置如下 YAML 所示：

```yaml
    spec:
      containers:
      - image: nginx:latest
        name: nginx
      nodeSelector:
        disksize: "big"
```





顺便聊一下官方的一个例子，设置 Node 的 Label，表示硬盘是 ssd。

```
kubectl label nodes node-1 disktype=ssd
```

在 yaml 文件的节点选择器中，添加选择。

```yaml
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
```

使用 Pod 表示：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
```



### 命令式标签选择

在前面，我们学习了 nodeSelector ，可以帮助我们选择合适的 Node 运行 Pod，实际上 Kubernets 的标签选择是丰富多样的，例如：

```yaml
  nodeSelector:
    disktype: ssd
    disksize: big
```

上面的例子表示节点选择器是等值选择，表达式是 `disktype=ssd && disksize=big`，节点需要具备这两个 Label (key和value都要符合)。



标签选择有等值和集合两种，其中等值选择有 `=`、`==`、`!=` 三种，`=` 和 `==` 无区别。所以实际只有 `等于` 、`不等于` 两种选择。

在多个需求(多个label)的情况下，可以使用 `&&` 运算符，表示需要同时符合多个条件，但是选择器不存在 `||` 这种逻辑或运算符。



yaml 只支持 `{key}:{value}` 这种形式，而我们使用命令形式时，则可使用以上三种运算符。

```shell
kubectl get nodes -l disktype=ssd,disksize!=big
# 多个条件使用 逗号","" 隔开，而不是 "&&"。
```



对于集合选择方式，支持三种操作符：`in`、`notin` 和 `exists`。不过别理解成是从集合中选择，下面举个例子。

假如有三个 Node，其 disksize 有 big、medium、small。

```
NAME        LABELS
node1      disksize=big
node2      disksize=medium
node3      disksize=small
```



我们要部署一个 pod，在 big、medium 中都可以运行，则：

```shell
... -l disksize in (big,medium)
kubectl get nodes -l disksize in (big,medium)
```

```shell
... -l disksize notin (small)
kubectl get nodes -l disksize notin (small)
# 不在 small 中运行
```

而 exists 则跟 `!=` 类似，但是 exists 表示只要存在这个 label 即可，而不论其设置了是什么值。

```shell
-l exists disksize
# 等同 -l disksize in (big,medium,small)
kubectl get nodes -l exists disksize
```



我们也可以使用 `''` 把选择表达式包起来。

```shell
kubectl get pods -l 'app=nginx'
```



### selector

前面已经提到了 yaml 的 nodeSelector 和 命令式的选择，这里我们介绍 yaml 的 selector。

前面我们提到在 Deployment 的 metadata 中加上 Label，即 pod 加上 Label，我们也在 `kubectl get pods` 中使用 Label 选择过滤 pod。同样，当我们创建 Service 或者使用 ReplicationController 时，也可以使用标签选择合适的 pod。



回忆一下，我们之前已经学习过怎么创建 Service，命令：

```
kubectl expose deployment xxx ...
```

命令式的选择很简单，加上 `deployment xxx` 即可。对于 YAML ，我们可以使用 `seletcor` 选择器，表示该 Service 在什么对象上起效。



假如我们已经部署了 nginx，那么查询 `kubectl get pods --show-labels` 时，其 pod 的 LABELS 会有 `app=nginx`，那么我们可以这样选择：

```
  selector:
    app: nginx
```

> 由 Deployment 部署的每个 Pod 的名称都不同，但是会为每个 Pod 加上一个相同的 label，表示它们都是同一个 Deployment 创建的。
>
> YAML 文件配置示例：
>
> ```yaml
>   template:
>     metadata:
>       labels:
>         app: nginx
> ```



使用 Service 时，要绑定具体的 Pod，完整的 YAML 示例如下：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 6666
status:
  loadBalancer:
    ingress:
      - ip: 192.0.2.127
```



### 选择器的复合运算

nodeSelector、selector 还支持以下选择方式 `matchLabels`、`matchExpressions`：

`matchLabels` 是由 `{key,value}` 对组成的映射。 `matchLabels` 映射中的单个 `{key,value }` 等同于 `matchExpressions` 的元素， 其 `key` 字段为 "key"，`operator` 为 "In"，而 `values` 数组仅包含 "value"。 

`matchExpressions` 是 Pod 选择算符需求的列表。 有效的运算符包括 `In`、`NotIn`、`Exists` 和 `DoesNotExist`。 在 `In` 和 `NotIn` 的情况下，设置的值必须是非空的。 来自 `matchLabels` 和 `matchExpressions` 的所有要求都按逻辑与的关系组合到一起 -- 它们必须都满足才能匹配。

示例如下：

```yaml
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}
```

这里就不在详细说这些选择规则了，前面提到的已经够用了，读者可以查阅官方文档学习更多复杂的操作：[https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/labels/)



## 节点调度

### 亲和性和反亲和性

前面我们学习了 `nodeSelector` ，使用 `nodeSelector` 选择合适的 节点部署 Pod。

亲和性类似于 nodeSelector，可以根据节点上的标签约束 pod 可以调度到哪些节点。

pod 亲和性有两种别为：

* `requiredDuringSchedulingIgnoredDuringExecution` 

  硬需求，将 pod 调度到一个节点必须满足的规则。

* `preferredDuringSchedulingIgnoredDuringExecution`。

  尝试执行但是不能保证偏好。

这是官方的一个例子：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: k8s.gcr.io/pause:2.0
```



亲和性的约束相对于：

```shell
... ... -l kubernetes.io/e2e-az-name in (e2e-az1,e2e-az2)
```

affinity 设置亲密关系，nodeAffinity 设置节点亲密关系，最后才到 亲和性，它们表示必须满足和尽量满足。

如果我们设置了多个 nodeSelectorTerms ：

```
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
  ...
  nodeSelectorTerms:
```

则只需要满足其中一种即可调度 pod 到 node 上。



如果你同时指定了 `nodeSelector` 和 `nodeAffinity`，*两者*必须都要满足， 才能将 Pod 调度到候选节点上。



节点亲和性语法支持下面的操作符： `In`，`NotIn`，`Exists`，`DoesNotExist`，`Gt`，`Lt`。

Pod 亲和性与反亲和性的合法操作符有 `In`，`NotIn`，`Exists`，`DoesNotExist`。



通过 `-Affinity` 可以设置亲和性，例如节点亲和性 `nodeAffinity`，而且设置反亲和性使用 `-AntiAffinity`，例如 `nodeAntiAffinity`。

反亲和性跟亲和性一样，都有 `requiredDuringSchedulingIgnoredDuringExecution` 硬限制和 `preferredDuringSchedulingIgnoredDuringExecution` 软限制，只是反亲和性是相反的表示，如果符合条件则不能调度。

关于亲和性和反亲和性的说明就到这里，着两者的配置比较多和复杂，读者可以参考官方文档，这里不在赘述。

> 亲和性和反亲和性的 YAML 很复杂，所以手写不出来的，只需要努力了解大概的意思和看懂就行，需要使用时查看文档。



### 污点和容忍度

#### 污点

前面提到亲和性和反亲和性，我们可以让 Pod 选择合适的 node，或者 service 选择合适的 Pod，这些拥有 Label 的对象都是被选择的。

```
Pod -选择-> 节点
```



这里，我们介绍污点和容忍度，它们可以排斥 “被选择” 的命运。

> 当节点添加一个污点后，除非 pod 声明能够容忍这个污点，否则 pod 不会被调度到这个 节点上。
>



节点污点(taint) 可以排斥一类特定的 Pod，而 容忍度(Tolerations)则表示能够容忍这个对象的污点。

```
Node - taint > 排斥 Pod
Pod  - tolerations -> 我是真爱，我能容忍污点 -> Node
```



污点有强制和尽量两种，前者完全排斥，后者尽可能排斥，另外污点可以将已经在这台节点上部署的 Pod 逐出，这个称为 effect。

> 系统会 尽量避免将 Pod 调度到存在其不能容忍污点的节点上， 但这不是强制的。Kubernetes 处理多个污点和容忍度的过程就像一个过滤器：从一个节点的所有污点开始遍历， 过滤掉那些 Pod 中存在与之相匹配的容忍度的污点。
>
> 但是如果你只有一个 worker，那么设置了污点，那 Pod 也只能选择在这个节点上运行。



添加污点格式：

```shell
kubectl taint node [node] key=value:[effect]
```

更新污点或覆盖：

```shell
kubectl taint node [node] key=value:[effect] --overwrite=true
```



使用 `kubectl taint` 给节点增加一个污点。

```shell
kubectl taint nodes node1 key1=value1:NoSchedule
```

移除污点：

```shell
kubectl taint nodes node1 key1=value1:NoSchedule-
```

其中，污点需要设置 label ，并设置这个 label 的效果为 NoSchedule。

污点的效果称为 effect ，节点的污点可以设置为以下三种效果：

- `NoSchedule`：不能容忍此污点的 Pod 不会被调度到节点上；不会影响已存在的 pod。
- `PreferNoSchedule`：Kubernetes 会避免将不能容忍此污点的 Pod 安排到节点上。
- `NoExecute`：如果 Pod 已在节点上运行，则会将该 Pod 从节点中逐出；如果尚未在节点上运行，则不会将其安排到节点上。



#### 系统默认污点

尽管一个节点上的污点完全排斥 Pod，但是某些系统创建的 Pod 可以容忍所有 `NoExecute` 和 `NoSchedule` 污点，因此不会被逐出，例如 master 节点是不能被部署 pod 的，但是 `kube-system` 命名空间却有很多系统 pod。当然通过修改污点，可以让 用户的 Pod 部署到 master 节点中。



查询所有节点的污点：

```shell
kubectl describe nodes | grep Taints
```

```
Taints:             node-role.kubernetes.io/master:NoSchedule
Taints:             key1=value1:NoSchedule
```

master 节点上会有一个 `node-role.kubernetes.io/master:NoSchedule` 的污点，因此处理系统 Pod，用户的 Pod 不会在此节点上部署。

这里我们做一个实践，去掉 master 节点的污点，使得 用户 Pod 能够在此 节点上部署。



我们去除 master 的污点：

```shell
kubectl taint node instance-1 node-role.kubernetes.io/master:NoSchedule-
# instance-1 是笔者的 master 节点名称
```



然后部署通过 Deployment 部署 Nginx，并设置副本集。

```shell
kubectl create deployment nginxtaint --image=nginx:latest --replicas=3
```

查看 pod：

```shell
kubectl get pods -o wide
```

结果笔者查到三个副本都在 master 节点上。

![1620119962(1)](./.images/1620119962(1).png)



这里只是练手，为了保证集群安全，我们需要恢复 master 的污点。

```shell
kubectl taint node instance-1 node-role.kubernetes.io/master:NoSchedule
```



当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：

- `node.kubernetes.io/not-ready`：节点未准备好。这相当于节点状态 `Ready` 的值为 "`False`"。
- `node.kubernetes.io/unreachable`：节点控制器访问不到节点. 这相当于节点状态 `Ready` 的值为 "`Unknown`"。
- `node.kubernetes.io/out-of-disk`：节点磁盘耗尽。
- `node.kubernetes.io/memory-pressure`：节点存在内存压力。
- `node.kubernetes.io/disk-pressure`：节点存在磁盘压力。
- `node.kubernetes.io/network-unavailable`：节点网络不可用。
- `node.kubernetes.io/unschedulable`: 节点不可调度。
- `node.cloudprovider.kubernetes.io/uninitialized`：如果 kubelet 启动时指定了一个 "外部" 云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。

如果我们的 Pod 对机器资源有要求，可以使用排斥相关的污点。



#### 容忍度

一个 node 可以设置污点，排斥 pod，但是 pod 也可以设置 容忍度，容忍 node 的污点。

YAML 示例：

```yaml
tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoSchedule"
```
也可以设置 value。
```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
```

> `operator` 的默认值是 `Equal`。



一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果，并且：

- 如果 `operator` 是 `Exists` 

  此时不需要填写 `value` 字段；如果存在 key 为 key1 的 label，且污点效果为 `NoSchedule`，无论是什么值都容忍。

- 如果 `operator` 是 `Equal` 

  则它们的 `value` 应该相等，如果相同的话，则容忍，如果只不同则容忍。

* 如果 `effect` 留空

  则表示只要是 label 为 `key1` 的节点，都可以容忍。

如果：

```shell
tolerations:
  operator: "Exists"
```

则表示此 pod 能够容忍任意的污点，无论 node 怎么设置 `key`、`value` 、`effect ` ，此 pod 都不会介意。



如果要在 master 上也能部署 pod，则可以修改 pod 的容忍度：

```yaml
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
```



### DaemonSet

在 Kubernetes 中，负载类型有 Deployments、ReplicaSet、DaemonSet、StatefulSets 等(或者说有这几个控制器)。

前面已经介绍过 Deployments 和 ReplicaSet，它们都可以通过命令和 YAML 创建，但是 ReplicaSet 一般没必要使用 YAML 创建的。



DaemonSet 可以确保一个节点只运行一个 Pod 副本。

假如有个 Pod，当新的 节点加入集群时，会自动在这个 节点 上部署一个 Pod；当节点从集群中移开时，这个 Node 上的 Pod 会被回收；如果 DaemontSet 配置被删除，则也会删除所有由它创建的 Pod。

DaemonSet 的一些典型用法：

- 在每个节点上运行集群守护进程
- 在每个节点上运行日志收集守护进程
- 在每个节点上运行监控守护进程

在 yaml 中，要配置 Daemont，可以使用 `tolerations`，配置示例：

```yaml
kind: DaemontSet
... ...
```

其它地方跟 Deployment 一致，这里据不多说了，读者了解即可。



关于 StatefulSets ，后面章节会提及。