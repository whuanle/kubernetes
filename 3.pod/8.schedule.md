## 节点调度

### 亲和性和反亲和性

前面我们学习了 `nodeSelector` ，使用 `nodeSelector` 选择合适的 节点部署 Pod。

亲和性类似于 nodeSelector，可以根据节点上的标签约束 pod 可以调度到哪些节点。

pod 亲和性有两种别为：

* `requiredDuringSchedulingIgnoredDuringExecution`

  硬需求，将 pod 调度到一个节点必须满足的规则。

* `preferredDuringSchedulingIgnoredDuringExecution`。

  尝试执行但是不能保证偏好。

这是官方的一个例子：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: k8s.gcr.io/pause:2.0
```

亲和性的约束相对于：

```text
... ... -l kubernetes.io/e2e-az-name in (e2e-az1,e2e-az2)
```

affinity 设置亲密关系，nodeAffinity 设置节点亲密关系，最后才到 亲和性，它们表示必须满足和尽量满足。

如果我们设置了多个 nodeSelectorTerms ：

```text
requiredDuringSchedulingIgnoredDuringExecution:
  nodeSelectorTerms:
  ...
  nodeSelectorTerms:
```

则只需要满足其中一种即可调度 pod 到 node 上。

如果你同时指定了 `nodeSelector` 和 `nodeAffinity`，_两者_必须都要满足， 才能将 Pod 调度到候选节点上。

节点亲和性语法支持下面的操作符： `In`，`NotIn`，`Exists`，`DoesNotExist`，`Gt`，`Lt`。

Pod 亲和性与反亲和性的合法操作符有 `In`，`NotIn`，`Exists`，`DoesNotExist`。

通过 `-Affinity` 可以设置亲和性，例如节点亲和性 `nodeAffinity`，而且设置反亲和性使用 `-AntiAffinity`，例如 `nodeAntiAffinity`。

反亲和性跟亲和性一样，都有 `requiredDuringSchedulingIgnoredDuringExecution` 硬限制和 `preferredDuringSchedulingIgnoredDuringExecution` 软限制，只是反亲和性是相反的表示，如果符合条件则不能调度。

关于亲和性和反亲和性的说明就到这里，着两者的配置比较多和复杂，读者可以参考官方文档，这里不在赘述。

> 亲和性和反亲和性的 YAML 很复杂，所以手写不出来的，只需要努力了解大概的意思和看懂就行，需要使用时查看文档。

### 污点和容忍度

#### 污点

前面提到亲和性和反亲和性，我们可以让 Pod 选择合适的 node，或者 service 选择合适的 Pod，这些拥有 Label 的对象都是被选择的。

```text
Pod -选择-> 节点
```

这里，我们介绍污点和容忍度，它们可以排斥 “被选择” 的命运。

> 当节点添加一个污点后，除非 pod 声明能够容忍这个污点，否则 pod 不会被调度到这个 节点上。

节点污点\(taint\) 可以排斥一类特定的 Pod，而 容忍度\(Tolerations\)则表示能够容忍这个对象的污点。

```text
Node - taint > 排斥 Pod
Pod  - tolerations -> 我是真爱，我能容忍污点 -> Node
```

污点有强制和尽量两种，前者完全排斥，后者尽可能排斥，另外污点可以将已经在这台节点上部署的 Pod 逐出，这个称为 effect。

> 系统会 尽量避免将 Pod 调度到存在其不能容忍污点的节点上， 但这不是强制的。Kubernetes 处理多个污点和容忍度的过程就像一个过滤器：从一个节点的所有污点开始遍历， 过滤掉那些 Pod 中存在与之相匹配的容忍度的污点。
>
> 但是如果你只有一个 worker，那么设置了污点，那 Pod 也只能选择在这个节点上运行。

添加污点格式：

```text
kubectl taint node [node] key=value:[effect]
```

更新污点或覆盖：

```text
kubectl taint node [node] key=value:[effect] --overwrite=true
```

使用 `kubectl taint` 给节点增加一个污点。

```text
kubectl taint nodes node1 key1=value1:NoSchedule
```

移除污点：

```text
kubectl taint nodes node1 key1=value1:NoSchedule-
```

其中，污点需要设置 label ，并设置这个 label 的效果为 NoSchedule。

污点的效果称为 effect ，节点的污点可以设置为以下三种效果：

* `NoSchedule`：不能容忍此污点的 Pod 不会被调度到节点上；不会影响已存在的 pod。
* `PreferNoSchedule`：Kubernetes 会避免将不能容忍此污点的 Pod 安排到节点上。
* `NoExecute`：如果 Pod 已在节点上运行，则会将该 Pod 从节点中逐出；如果尚未在节点上运行，则不会将其安排到节点上。

#### 系统默认污点

尽管一个节点上的污点完全排斥 Pod，但是某些系统创建的 Pod 可以容忍所有 `NoExecute` 和 `NoSchedule` 污点，因此不会被逐出，例如 master 节点是不能被部署 pod 的，但是 `kube-system` 命名空间却有很多系统 pod。当然通过修改污点，可以让 用户的 Pod 部署到 master 节点中。

查询所有节点的污点：

```text
kubectl describe nodes | grep Taints
```

```text
Taints:             node-role.kubernetes.io/master:NoSchedule
Taints:             key1=value1:NoSchedule
```

master 节点上会有一个 `node-role.kubernetes.io/master:NoSchedule` 的污点，因此处理系统 Pod，用户的 Pod 不会在此节点上部署。

这里我们做一个实践，去掉 master 节点的污点，使得 用户 Pod 能够在此 节点上部署。

我们去除 master 的污点：

```text
kubectl taint node instance-1 node-role.kubernetes.io/master:NoSchedule-
# instance-1 是笔者的 master 节点名称
```

然后部署通过 Deployment 部署 Nginx，并设置副本集。

```text
kubectl create deployment nginxtaint --image=nginx:latest --replicas=3
```

查看 pod：

```text
kubectl get pods -o wide
```

结果笔者查到三个副本都在 master 节点上。

!\[1620119962\(1\)\]\(./.images/1620119962\(1\).png\)

这里只是练手，为了保证集群安全，我们需要恢复 master 的污点。

```text
kubectl taint node instance-1 node-role.kubernetes.io/master:NoSchedule
```

当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：

* `node.kubernetes.io/not-ready`：节点未准备好。这相当于节点状态 `Ready` 的值为 "`False`"。
* `node.kubernetes.io/unreachable`：节点控制器访问不到节点. 这相当于节点状态 `Ready` 的值为 "`Unknown`"。
* `node.kubernetes.io/out-of-disk`：节点磁盘耗尽。
* `node.kubernetes.io/memory-pressure`：节点存在内存压力。
* `node.kubernetes.io/disk-pressure`：节点存在磁盘压力。
* `node.kubernetes.io/network-unavailable`：节点网络不可用。
* `node.kubernetes.io/unschedulable`: 节点不可调度。
* `node.cloudprovider.kubernetes.io/uninitialized`：如果 kubelet 启动时指定了一个 "外部" 云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。

如果我们的 Pod 对机器资源有要求，可以使用排斥相关的污点。

#### 容忍度

一个 node 可以设置污点，排斥 pod，但是 pod 也可以设置 容忍度，容忍 node 的污点。

YAML 示例：

```yaml
tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoSchedule"
```

也可以设置 value。

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
```

> `operator` 的默认值是 `Equal`。

一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果，并且：

* 如果 `operator` 是 `Exists`

  此时不需要填写 `value` 字段；如果存在 key 为 key1 的 label，且污点效果为 `NoSchedule`，无论是什么值都容忍。

* 如果 `operator` 是 `Equal`

  则它们的 `value` 应该相等，如果相同的话，则容忍，如果只不同则容忍。

* 如果 `effect` 留空

  则表示只要是 label 为 `key1` 的节点，都可以容忍。

如果：

```text
tolerations:
  operator: "Exists"
```

则表示此 pod 能够容忍任意的污点，无论 node 怎么设置 `key`、`value` 、`effect` ，此 pod 都不会介意。

如果要在 master 上也能部署 pod，则可以修改 pod 的容忍度：

```yaml
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
```

### 